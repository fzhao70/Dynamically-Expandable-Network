# Dynamically Expandable Network (DEN)

A PyTorch implementation of **Dynamically Expandable Neural Networks** that can automatically grow their architecture during training to accommodate complex patterns and new data.

## ğŸŒŸ Features

- **Automatic Growth**: Networks expand automatically based on training dynamics
- **Width Expansion**: Add neurons to existing layers when needed
- **Depth Expansion**: Add new layers to increase network capacity
- **Continual Learning**: Learn from new data streams without forgetting
- **Multiple Growth Strategies**: Choose from loss-based, gradient-based, adaptive, or **biological growth**
- **Biological Growth** ğŸ§  **NEW!**: Mimics real neural development in living creatures
  - Activity-dependent neurogenesis
  - Hebbian learning principles
  - Synaptic pruning of weak neurons
  - Energy efficiency optimization
- **AdamW Optimizer**: Improved optimizer with weight decay
- **Checkpoint/Restart**: Save and resume training at any point
- **PyTorch Native**: Built entirely with PyTorch for seamless integration

## ğŸ“¦ Installation

### From Source

```bash
git clone https://github.com/yourusername/Dynamically-Expandable-Network.git
cd Dynamically-Expandable-Network
pip install -e .
```

### Requirements

```bash
pip install -r requirements.txt
```

**Dependencies:**
- Python >= 3.8
- PyTorch >= 2.0.0
- NumPy >= 1.21.0
- Matplotlib >= 3.5.0
- scikit-learn >= 1.0.0

## ğŸš€ Quick Start

### Basic Usage

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from den import DynamicExpandableNetwork, DENTrainer, LossBasedGrowth
from den.utils import plot_training_history

# Create your data
X_train = torch.randn(1000, 10)
y_train = torch.randn(1000, 1)
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)

# Create a DEN with initial small architecture
network = DynamicExpandableNetwork(
    input_size=10,
    output_size=1,
    hidden_sizes=[16, 16],  # Start small!
    activation=nn.ReLU,     # Pass nn.Module class directly
    task_type='regression'
)

# Set up growth strategy
growth_strategy = LossBasedGrowth(
    patience=10,      # Wait 10 epochs before growing
    cooldown=5,       # Wait 5 epochs after growth
    min_delta=1e-4    # Minimum improvement threshold
)

# Create trainer
trainer = DENTrainer(
    network=network,
    growth_strategy=growth_strategy,
    optimizer=torch.optim.Adam,  # Pass optimizer class directly
    learning_rate=0.001
)

# Train with automatic growth
history = trainer.train(
    train_loader=train_loader,
    epochs=100,
    enable_growth=True
)

print(f"Final architecture: {network.get_layer_sizes()}")
print(f"Total parameters: {network.get_num_parameters()}")

# NEW: Plot with wall-clock time on x-axis
plot_training_history(history, save_path='training_time.png', use_time=True)
```

### Continual Learning

```python
# Initial training
trainer.train(train_loader_1, epochs=50)

# Learn from new data (network grows automatically if needed)
trainer.continual_learning(
    new_data=X_new,
    new_targets=y_new,
    epochs=20
)
```

### Save and Restart

```python
# Save checkpoint
trainer.save_checkpoint('checkpoint.pt')

# Later... restart training
trainer.load_checkpoint('checkpoint.pt')
trainer.train(new_train_loader, epochs=50)
```

## ğŸ“– Core Components

### 1. DynamicExpandableNetwork

The main network class that supports dynamic architecture changes.

```python
network = DynamicExpandableNetwork(
    input_size=10,           # Number of input features
    output_size=1,           # Number of outputs
    hidden_sizes=[32, 32],   # Initial hidden layer sizes
    activation=nn.ReLU,      # Activation function (nn.Module class)
    dropout=0.1,            # Dropout rate (optional)
    task_type='regression'  # 'regression' or 'classification'
)

# You can also use:
# activation=nn.Tanh
# activation=nn.GELU
# activation=nn.LeakyReLU
# activation=lambda: nn.ReLU(inplace=True)
```

**Key Methods:**
- `expand_layer_width(layer_idx, num_neurons)`: Add neurons to a layer
- `expand_depth(position, num_neurons)`: Add a new layer
- `save_checkpoint(path)`: Save network state
- `load_checkpoint(path)`: Load network state

### 2. Growth Strategies

Control when and how the network grows.

#### LossBasedGrowth

Grows when loss plateaus:

```python
from den import LossBasedGrowth

strategy = LossBasedGrowth(
    patience=15,                    # Epochs to wait for improvement
    cooldown=5,                     # Epochs to wait after growth
    min_delta=1e-4,                # Improvement threshold
    width_growth_ratio=0.5,        # Add 50% more neurons
    max_neurons_per_expansion=32   # Max neurons to add at once
)
```

#### GradientBasedGrowth

Grows based on gradient magnitudes:

```python
from den import GradientBasedGrowth

strategy = GradientBasedGrowth(
    patience=10,
    gradient_threshold=0.1,  # Threshold for gradient magnitude
    max_neurons_per_expansion=32
)
```

#### AdaptiveGrowth

Combines multiple signals (recommended for general use):

```python
from den import AdaptiveGrowth

strategy = AdaptiveGrowth(
    patience=15,
    cooldown=10,
    loss_threshold=1e-4,
    gradient_threshold=0.05,
    max_network_size=10000  # Prevent unlimited growth
)
```

#### BiologicalGrowth

**NEW!** Mimics neural development in living organisms:

```python
from den import BiologicalGrowth

strategy = BiologicalGrowth(
    patience=12,
    cooldown=8,
    activity_threshold=0.3,      # Neuron activity trigger
    pruning_threshold=0.1,       # Prune weak neurons
    energy_cost_weight=0.01,     # Metabolic cost penalty
    max_neurons_per_expansion=16,
    enable_pruning=True,         # Enable synaptic pruning
    hebbian_window=5             # Activity correlation window
)
```

**Biological Principles:**
- **Activity-dependent neurogenesis**: Active neurons promote growth
- **Hebbian learning**: "Neurons that fire together wire together"
- **Synaptic pruning**: Removes weak/unused neurons
- **Energy efficiency**: Balances performance vs. network size
- **Competitive growth**: Resources allocated to active regions

Perfect for applications mimicking brain development and adaptive learning!

### 3. DENTrainer

Handles training with automatic growth.

```python
trainer = DENTrainer(
    network=network,
    growth_strategy=strategy,
    optimizer=torch.optim.AdamW,  # Pass optimizer class directly
    optimizer_kwargs={'weight_decay': 0.01},  # Optional optimizer parameters
    learning_rate=0.001,
    device='cuda',             # 'cuda' or 'cpu'
    verbose=True
)

# You can use any PyTorch optimizer:
# optimizer=torch.optim.Adam
# optimizer=torch.optim.SGD (with optimizer_kwargs={'momentum': 0.9})
# optimizer=torch.optim.RMSprop
```

**Key Methods:**
- `train(train_loader, epochs, enable_growth=True)`: Train with growth
- `evaluate(data_loader)`: Evaluate performance
- `continual_learning(new_data, new_targets)`: Learn from new data
- `predict(data)`: Make predictions

**New in v0.2:**
- Training history includes timestamps (`timestamps` field)
- Growth events include `timestamp` and `datetime` fields
- Plot training vs wall-clock time with `use_time=True`

## ğŸ“Š Visualization and Analysis

```python
from den.utils import (
    plot_training_history,
    print_growth_summary,
    visualize_network_architecture
)

# Plot training progress and growth events
plot_training_history(history, save_path='training.png')

# Print detailed growth summary
print_growth_summary(history)

# Visualize final architecture
visualize_network_architecture(network, save_path='architecture.png')
```

## ğŸ’¡ Examples

### Example 1: Simple Regression

```bash
cd examples
python simple_regression.py
```

Demonstrates basic usage with synthetic regression data.

### Example 2: Continual Learning

```bash
python continual_learning.py
```

Shows how to learn multiple tasks sequentially with automatic growth.

### Example 3: Classification

```bash
python classification_example.py
```

Multi-class classification with dynamic architecture.

### Example 4: Biological Growth (NEW!)

```bash
python biological_growth_example.py
```

**Mimics how real brains develop!** Watch the network grow through developmental stages:
- **Infant stage**: Learning simple patterns with basic neural circuits
- **Child stage**: Developing complex representations
- **Adult stage**: Mastering abstract concepts

Features biological principles:
- Activity-dependent neurogenesis (neurons grow where needed)
- Hebbian learning (firing together = wiring together)
- Synaptic pruning (weak connections removed)
- Energy efficiency optimization

This is the most realistic simulation of how living brains learn and adapt!

## ğŸ—ï¸ Architecture

```
Dynamically-Expandable-Network/
â”œâ”€â”€ den/
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ core.py               # DynamicExpandableNetwork class
â”‚   â”œâ”€â”€ layers.py             # ExpandableLinear layer
â”‚   â”œâ”€â”€ growth_strategy.py    # Growth decision strategies
â”‚   â”œâ”€â”€ trainer.py            # DENTrainer class
â”‚   â””â”€â”€ utils.py              # Visualization utilities
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ simple_regression.py
â”‚   â”œâ”€â”€ continual_learning.py
â”‚   â””â”€â”€ classification_example.py
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ“ Growth Strategy Mathematics

This section provides detailed mathematical formulations for each growth strategy.

### LossBasedGrowth - Mathematical Formulation

**Core Principle**: Plateau Detection
Triggers growth when training loss stops improving for a sustained period.

**Decision Criteria:**

1. **Loss Improvement Check:**
   ```
   Î”â‚œ = L_best - L_t

   where:
   - L_t: Current loss at epoch t
   - L_best: Best loss observed so far
   - Î´: Minimum improvement threshold (min_delta)

   Improvement condition: Î”â‚œ > Î´
   ```

2. **Patience Counter:**
   ```
   p_t = {
       0,           if Î”â‚œ > Î´
       p_{t-1} + 1, otherwise
   }

   Growth trigger: p_t â‰¥ P
   where P = patience parameter
   ```

3. **Cooldown Period:**
   ```
   c_t = epochs since last growth

   Allow growth only if: c_t â‰¥ C
   where C = cooldown parameter
   ```

**Growth Magnitude:**

For **width expansion**:
```
n_new = min(âŒŠn_current Ã— râŒ‹, n_max)

where:
- n_current: Current layer size
- r: width_growth_ratio (default 0.5)
- n_max: max_neurons_per_expansion
```

For **depth expansion** (triggered every `depth_threshold` width expansions):
```
n_new = min(mean(layer_sizes), n_max)
position = âŒŠnum_layers / 2âŒ‹
```

**Layer Selection for Width Expansion:**
```
i* = argmin{|L_i|}

where L_i is the size of layer i
```
Choose smallest layer to balance architecture.

---

### GradientBasedGrowth - Mathematical Formulation

**Core Principle**: Under-Parameterization Detection
Monitors gradient magnitudes to identify layers struggling to learn.

**Gradient Tracking:**

1. **Layer-wise Gradient Norm:**
   ```
   g_i^(t) = ||âˆ‡_W_i L||â‚‚

   where:
   - W_i: Weights of layer i
   - L: Loss function
   - || Â· ||â‚‚: L2 norm
   ```

2. **Temporal Gradient History:**
   ```
   G_i = {g_i^(t-k), g_i^(t-k+1), ..., g_i^(t)}

   á¸¡_i = (1/k) Î£ g_i^(Ï„)

   where k = hebbian_window
   ```

3. **Network-Level Gradient:**
   ```
   á¸¡ = (1/N) Î£áµ¢ á¸¡_i

   where N = number of layers
   ```

**Growth Decision:**

Grow when BOTH conditions are met:

```
Condition 1: á¸¡ > Î¸_g  (high gradients)
Condition 2: p_t â‰¥ P  (loss plateau)

where:
- Î¸_g: gradient_threshold
- p_t: patience counter (as in LossBasedGrowth)
```

**Layer Selection:**
```
i* = argmax{á¸¡_i}
```
Expand the layer with highest average gradient (most struggling).

**Growth Magnitude:**
```
n_new = min(max(âŒŠn_i* Ã— 0.3âŒ‹, 1), n_max)
```

---

### AdaptiveGrowth - Mathematical Formulation

**Core Principle**: Multi-Signal Integration
Combines loss, gradients, and network capacity for intelligent growth decisions.

**Loss Analysis:**

1. **Loss Standard Deviation:**
   ```
   Ïƒ_L = âˆš[(1/P) Î£ (L_t - LÌ„)Â²]

   where LÌ„ = (1/P) Î£ L_t over last P epochs
   ```

2. **Loss Improvement Rate:**
   ```
   Î”L = L_{t-P} - L_t
   ```

3. **Stagnation Detection:**
   ```
   stagnant = (Î”L < Î¸_L) âˆ§ (Ïƒ_L < Î¸_L)

   where Î¸_L = loss_threshold
   ```

**Network Efficiency:**

```
E_t = 1 / (L_t Ã— (1 + n_params/1000))

where:
- E_t: Efficiency at time t
- n_params: Total network parameters
```

Penalizes large networks with poor performance.

**Plasticity Detection:**

```
needs_plasticity = (á¸¡ > Î¸_g) âˆ¨ (E_t < 0.8 Ã— Ä’)

where:
- á¸¡: Average gradient norm
- Î¸_g: gradient_threshold
- Ä’: Mean efficiency over recent epochs
```

**Growth Decision:**
```
grow = (stagnant âˆ¨ needs_plasticity) âˆ§ (p_t â‰¥ P) âˆ§ (n_params < n_max)
```

**Intelligent Layer Selection:**

Combined score for each layer:
```
S_i = (a_i / (n_i + 1)) Ã— (1 + Î±_i)

where:
- a_i: Activity/gradient score
- n_i: Current layer size
- Î±_i: Activation magnitude
```

**Growth Type Decision:**

```
type = {
    depth,  if (N < 3) âˆ¨ (Var(layer_sizes) < Î¼ Ã— 0.1)
    width,  otherwise
}

where:
- N: Number of layers
- Î¼: Mean layer size
- Var: Variance of layer sizes
```

**Depth Growth:**
```
position = âŒŠN/2âŒ‹
n_new = min(Î¼, n_max)
```

**Width Growth:**
```
i* = argmax{S_i}
n_new = min(âŒŠn_i* Ã— 0.5âŒ‹, n_max)
```

---

### BiologicalGrowth - Mathematical Formulation

**Core Principle**: Biomimetic Neural Development
Simulates biological neurogenesis, pruning, and metabolic constraints.

**Neuron Activity (Firing Rate Analog):**

1. **Layer Activity:**
   ```
   A_i = Î£â±¼ |w_ij| + |b_i|

   Normalized: Ã¢_i = A_i / max(A)

   where:
   - w_ij: Weight from neuron j to i
   - b_i: Bias of neuron i
   ```

2. **Temporal Activity:**
   ```
   Ä€_i^(t) = (1/k) Î£_{Ï„=t-k}^t Ã¢_i^(Ï„)

   where k = hebbian_window
   ```

**Energy Cost (Metabolic Constraint):**

```
E_metabolic = (n_params^1.2) Ã— w_energy

where:
- n_params: Total parameters
- w_energy: energy_cost_weight
- 1.2 exponent: Nonlinear scaling (like brain metabolism)
```

**Network Efficiency:**

```
Î· = 1 / (L Ã— (1 + n_params/1000))

where:
- L: Current loss
- Î·: Efficiency (performance per parameter)
```

**Efficiency History:**

```
H_Î· = {Î·_{t-k}, ..., Î·_t}

Declining efficiency: Î·_t < 0.8 Ã— mean(H_Î·)
```

**Plasticity Need Detection:**

```
plasticity_needed = (á¸¡ > 0.05) âˆ¨ (declining_efficiency)

where á¸¡ = average gradient norm
```

**Activity-Dependent Growth:**

High sustained activity triggers neurogenesis:
```
high_activity = Ä€_overall > Î¸_A

where:
- Ä€_overall = mean(Ä€_i) across all layers
- Î¸_A: activity_threshold
```

**Growth Decision (Hebbian-Inspired):**

```
grow = (plasticity_needed âˆ¨ high_activity) âˆ§
       (p_t â‰¥ P) âˆ§
       (c_t â‰¥ C) âˆ§
       (n_params < n_max)
```

**Competitive Resource Allocation:**

Score for each layer (competitive growth):
```
S_i = (2 Ã— Ä€_i) + (1/(n_i + 1))

where:
- Ä€_i: Activity score (favor active regions)
- n_i: Layer size (favor small layers)
```

Target layer: `i* = argmax{S_i}`

**Growth Magnitude (Activity-Dependent):**

```
n_base = max(âŒŠn_i* Ã— 0.3âŒ‹, 2)
n_bonus = âŒŠn_base Ã— Ä€_i*âŒ‹
n_new = min(n_base + n_bonus, n_max)
```

More active layers â†’ more new neurons (like BDNF signaling).

**Depth Growth (Cortical Development):**

Triggered when activity is very high:
```
if max(Ä€_i) > 0.7 and N < 6:
    Add layer at position âŒŠN/2âŒ‹
```

**Synaptic Pruning:**

1. **Neuron Importance:**
   ```
   I_j = ||w_Â·j||â‚‚ + |b_j|

   Normalized: Ã®_j = I_j / max(I)
   ```

2. **Pruning Criterion:**
   ```
   prune_j = (Ã®_j < Î¸_prune) âˆ§ (n_i > n_min)

   where:
   - Î¸_prune: pruning_threshold
   - n_min: Minimum layer size (e.g., 8)
   ```

3. **Pruning Frequency:**
   ```
   Can prune if: (t - t_last_prune) > 2C

   where C = cooldown
   ```

**"Use it or Lose it" Principle:**

If >20% of layer's neurons have low activity:
```
weak_fraction = |{j : Ã®_j < Î¸_prune}| / n_i

if weak_fraction > 0.2:
    Trigger pruning
```

---

## ğŸ”¬ How It Works

### Width Expansion

When a layer needs more capacity, DEN adds neurons:

1. Creates a new layer with additional neurons
2. Copies existing weights
3. Initializes new neuron weights (Kaiming/Xavier)
4. Updates subsequent layer input dimensions

### Depth Expansion

When the network needs more expressiveness, DEN adds layers:

1. Determines optimal position for new layer
2. Creates layer with appropriate dimensions
3. Adjusts connections to maintain gradient flow
4. Initializes weights to preserve learned knowledge

### Growth Triggers

Networks grow when:
- Loss stops improving (plateau detection)
- Gradients remain high (under-parameterization)
- Capacity utilization is high
- New patterns don't fit current architecture

## ğŸ¯ Use Cases

### 1. Continual Learning
Learn from data streams without catastrophic forgetting:
```python
for task_data in data_stream:
    trainer.continual_learning(task_data, epochs=20)
```

### 2. Online Learning
Adapt to new patterns in real-time:
```python
while new_data_available():
    trainer.continual_learning(get_new_batch(), epochs=5)
```

### 3. AutoML
Automatically find optimal architecture:
```python
# Start small, let it grow to the right size
network = DynamicExpandableNetwork(
    input_size=features,
    output_size=targets,
    hidden_sizes=[8, 8]  # Very small initial size
)
```

### 4. Transfer Learning
Grow network for new tasks while preserving old knowledge.

## âš™ï¸ Advanced Configuration

### Custom Growth Strategy

```python
from den.growth_strategy import GrowthStrategy

class MyCustomGrowth(GrowthStrategy):
    def should_grow(self, metrics, network, epoch):
        # Your custom logic
        if metrics['loss'] > threshold:
            return True, "Custom reason"
        return False, None

    def determine_growth_action(self, metrics, network):
        # Decide how to grow
        return {
            'type': 'width',
            'layer_idx': 0,
            'num_neurons': 16
        }
```

### Custom Loss Function

```python
import torch.nn as nn

trainer = DENTrainer(
    network=network,
    loss_function=nn.HuberLoss(),  # Custom loss
    ...
)
```

## ğŸ“ˆ Performance Tips

1. **Start Small**: Begin with small architectures (8-16 neurons)
2. **Tune Patience**: Higher patience = fewer, larger growths
3. **Set Max Size**: Use `max_network_size` to prevent unlimited growth
4. **Use Adaptive Strategy**: Best for most use cases
5. **Monitor Growth**: Use visualization tools to understand growth patterns

## ğŸ” Monitoring

```python
# During training
print(f"Current architecture: {network.get_layer_sizes()}")
print(f"Total parameters: {network.get_num_parameters()}")
print(f"Growth events: {len(history['growth_events'])}")

# Layer importance analysis
from den.utils import analyze_layer_importance
importance = analyze_layer_importance(network)
```

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# With coverage
pytest tests/ --cov=den --cov-report=html
```

## ğŸ“š Research Background

This implementation is inspired by:
- Dynamic Expandable Networks for continual learning
- Progressive Neural Networks
- Neural Architecture Search
- Lifelong learning systems

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional growth strategies
- Pruning capabilities
- More visualization tools
- Additional examples
- Performance optimizations

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- PyTorch team for the excellent framework
- Research community for continual learning insights

## ğŸ“§ Contact

For questions, issues, or suggestions:
- Open an issue on GitHub
- Submit a pull request

## ğŸ—ºï¸ Roadmap

- [ ] Pruning capabilities
- [ ] Multi-task learning support
- [ ] Automated hyperparameter tuning
- [ ] More growth strategies
- [ ] Integration with popular datasets
- [ ] Distributed training support
- [ ] ONNX export support

---

**Start with a small network and let it grow!** ğŸŒ±â¡ï¸ğŸŒ³
